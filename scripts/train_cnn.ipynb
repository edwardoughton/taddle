{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file does two things: training a CNN per country, or training a CNN on a subset of countries to predict another. Ideally, we would do a 5-fold cross-validation and train the CNN per fold. But for 2 countries, 5 folds, and 2 metrics this leads to 40 CNN training runs. Furthermore, there are different types of cross-validation (randomized or spatial with regard to clusters). That would mean 40 CNN training runs. And this doesn't even count training the CNN again for cross-country generalization tests. The approach taken by the paper and in this file reduces the runs to (2 per country using 70/30 train/valid + 2 holding one country out) * 2 metrics = 8 CNN runs. This is far more manageable and less prone to error.\n",
    "<br> <br>\n",
    "Written by Jatin Mathur\n",
    "<br>\n",
    "5/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "BASE_DIR = '..'\n",
    "import sys\n",
    "sys.path.append(BASE_DIR)\n",
    "from utils import merge_on_lat_lon\n",
    "from config import TRAINING_CONFIG, RANDOM_SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRIES_DIR = os.path.join(BASE_DIR, 'data', 'countries')\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, 'data', 'processed')\n",
    "# can try using the google downloader, in which case change this to be your google api token\n",
    "ACCESS_TOKEN_DIR = os.path.join(BASE_DIR, 'planet_api_key.txt')\n",
    "\n",
    "TYPE = TRAINING_CONFIG['TYPE']\n",
    "COUNTRY = TRAINING_CONFIG['COUNTRY']\n",
    "METRIC = TRAINING_CONFIG['METRIC']\n",
    "\n",
    "CNN_TRAIN_IMAGE_DIR = os.path.join(BASE_DIR, 'data', 'cnn_images', TYPE, COUNTRY, METRIC)\n",
    "CNN_SAVE_DIR = os.path.join(BASE_DIR, 'models', TYPE, COUNTRY, METRIC)\n",
    "\n",
    "# groups to cut distribution into\n",
    "NUMBER_OF_BINS = 4 \n",
    "\n",
    "# reduce if memory errors on CUDA\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Number of epochs to train for\n",
    "# after epoch 5, the model will update the entire network (not just the newly initialized ones)\n",
    "TOTAL_EPOCHS = 30\n",
    "# if script notices existing models at earlier epochs, it will load that and set this variable\n",
    "CURRENT_EPOCH = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert TYPE in ['single_country', 'country_held_out']\n",
    "assert COUNTRY in ['malawi_2016', 'ethiopia_2015']\n",
    "assert METRIC in ['house_has_cellphone', 'est_monthly_phone_cost_pc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(CNN_TRAIN_IMAGE_DIR, exist_ok=True)\n",
    "os.makedirs(CNN_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(PROCESSED_DIR, TYPE, COUNTRY), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected: 26200 actually downloaded: 26155\n"
     ]
    }
   ],
   "source": [
    "DF_DOWNLOAD = pd.read_csv(os.path.join(PROCESSED_DIR, 'image_download_locs.csv'))\n",
    "downloaded = os.listdir(os.path.join(COUNTRIES_DIR, 'malawi_2016', 'cnn_images')) + \\\n",
    "            os.listdir(os.path.join(COUNTRIES_DIR, 'ethiopia_2015', 'cnn_images'))\n",
    "\n",
    "print(\"expected:\", len(DF_DOWNLOAD), \"actually downloaded:\", len(downloaded))\n",
    "\n",
    "# it's not that bad if some don't download, we just drop them from consideration\n",
    "DF_DOWNLOAD['row'] = np.arange(len(DF_DOWNLOAD))\n",
    "idx_not_download = DF_DOWNLOAD.set_index('image_name').drop(downloaded)['row'].values.tolist()\n",
    "DF_DOWNLOAD.drop(idx_not_download, inplace=True)\n",
    "DF_DOWNLOAD.drop('row', axis=1, inplace=True)\n",
    "DF_DOWNLOAD.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_bin(cutoffs):\n",
    "    '''\n",
    "    Returns a function that takes a scalar value x and assigns it to a bin based on \n",
    "    the cutoffs given to the \"parent\" function\n",
    "    '''\n",
    "    def binning_function(x):\n",
    "        # inner_function is still aware of variable cutoffs\n",
    "        for i in range(len(cutoffs) - 1):\n",
    "            if (x >= cutoffs[i]).any() and (x < cutoffs[i + 1]).any():\n",
    "                return i\n",
    "        raise ValueError(f'Given value {x} is outside the cutoffs')\n",
    "    return binning_function\n",
    "\n",
    "def create_bin_eth_house_has_cellphone(df, metric):\n",
    "    '''\n",
    "    This is a hacky fix for the odd case that doesn't work:\n",
    "    when the country is ethiopia and metric is house_has_cellphone\n",
    "    \n",
    "    pd.qcut will not work because 28% of the data is all 1's, meaning no range that\n",
    "    includes '1' can have only 25% of the data.\n",
    "    I choose hand-determined cutoffs that work for this country and this metric\n",
    "    \n",
    "    In general pd.qcut will fail whenever a single value dominates the distribution. This may lead to\n",
    "    more preprocessing steps for this method to generalize to other countries in a consistent way\n",
    "    '''\n",
    "    hand_cutoffs = np.array([0, 0.3, 0.6, 0.9, 1.01])\n",
    "    binning_function = assign_bin(hand_cutoffs)\n",
    "    bin_assignment = df[metric].apply(binning_function)\n",
    "    return bin_assignment, hand_cutoffs\n",
    "\n",
    "def create_bin(df, metric):\n",
    "    '''\n",
    "    df: dataframe with column metric\n",
    "    \n",
    "    Uses a quantile cut to bin the metric of interest into four equally-represented categories\n",
    "    Also identifies the images that are near the lower and upper cutoffs\n",
    "    \n",
    "    Adds columns 'bin', 'near_upper', and 'near_lower' to df\n",
    "    '''\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    frac_lower = 0.1 # lower 10% of a bin's range will count as being \"near\"\n",
    "    frac_upper = 0.1 # upper 10% of a bin's range will count as being \"near\"\n",
    "    df['bin'] = 0\n",
    "    bin_cutoffs = None\n",
    "    if TYPE == 'single_country' and COUNTRY == 'ethiopia_2015' and metric == 'house_has_cellphone':\n",
    "        # special case, function explains why\n",
    "        df['bin'], bin_cutoffs = create_bin_eth_house_has_cellphone(df, metric)\n",
    "    else: \n",
    "        bins, bin_cutoffs = pd.qcut(df[metric], NUMBER_OF_BINS, retbins=True)\n",
    "        df['bin'] = bins.cat.codes\n",
    "    df['bin'] = df['bin'].astype(np.int64)\n",
    "    df['near_lower'] = False\n",
    "    df['near_upper'] = False\n",
    "    for i in range(1, len(bin_cutoffs) - 1):\n",
    "        span = bin_cutoffs[i + 1] - bin_cutoffs[i]\n",
    "        if i != 0:\n",
    "            # we take the minimum of the current bin and the bin \n",
    "            # we want to join to as the effective span\n",
    "            # this prevents a bin with very large span from dominating\n",
    "            span = min(span, bin_cutoffs[i] - bin_cutoffs[i - 1])\n",
    "        lower_c = bin_cutoffs[i] + frac_lower * span\n",
    "        df['near_lower'].loc[(df['bin'] == i) & (df[metric] < lower_c)] = True\n",
    "    for i in range(0, len(bin_cutoffs) - 2):\n",
    "        span = bin_cutoffs[i + 1] - bin_cutoffs[i]\n",
    "        if i != len(bin_cutoffs) - 2:\n",
    "            # we take the minimum of the current bin and the bin \n",
    "            # we want to join to as the effective span\n",
    "            # this prevents a bin with very large span from dominating\n",
    "            span = min(span, bin_cutoffs[i + 2] - bin_cutoffs[i + 1])\n",
    "        upper_c = bin_cutoffs[i + 1] - frac_upper * span\n",
    "        df['near_upper'].loc[(df['bin'] == i) & (df[metric] > upper_c)] = True\n",
    "    \n",
    "\n",
    "def preprocess_single_country(frac=0.7):\n",
    "    '''\n",
    "    uses DF_DOWNLOAD and given country to hold out\n",
    "    frac represents the percent of clusters to use for training\n",
    "    \n",
    "    saves the images (symlinked) to data/cnn_images/TYPE/COUNTRY/\n",
    "    saves the dataframe to data/processed/TYPE/COUNTRY/METRIC.csv\n",
    "    '''\n",
    "    savedir = os.path.join(PROCESSED_DIR, TYPE, COUNTRY)\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "    savepath = os.path.join(savedir, f'{METRIC}.csv')\n",
    "    if os.path.exists(savepath):\n",
    "        print(\"already processed this country\")\n",
    "        df_images = pd.read_csv(savepath)\n",
    "        return df_images\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    df_images = DF_DOWNLOAD[DF_DOWNLOAD['country'] == COUNTRY].copy()\n",
    "    unique_clusters = df_images[['cluster_lat', 'cluster_lon']].drop_duplicates()\n",
    "    shuffled_clusters = unique_clusters.sample(frac=1)\n",
    "    num_train = int(frac * len(shuffled_clusters))\n",
    "    train_clusters = shuffled_clusters[:num_train]\n",
    "    train_clusters['is_train'] = True\n",
    "    df_images = merge_on_lat_lon(df_images, train_clusters, how='left')\n",
    "    # if not marked as true, will be NA (aka a validation cluster)\n",
    "    df_images['is_train'].fillna(False, inplace=True)\n",
    "    create_bin(df_images, METRIC)\n",
    "    \n",
    "    os.makedirs(os.path.join(CNN_TRAIN_IMAGE_DIR, 'train'), exist_ok=False)\n",
    "    os.makedirs(os.path.join(CNN_TRAIN_IMAGE_DIR, 'valid'), exist_ok=False)\n",
    "\n",
    "    symlink_images(df_images)\n",
    "    \n",
    "    # save to disk\n",
    "    df_images.to_csv(savepath, index=False)\n",
    "    return df_images\n",
    "\n",
    "def preprocess_country_held_out():\n",
    "    '''\n",
    "    uses DF_DOWNLOAD and holds given country out\n",
    "    '''\n",
    "    savepath = os.path.join(PROCESSED_DIR, TYPE, COUNTRY, f'{METRIC}.csv')\n",
    "    if os.path.exists(savepath):\n",
    "        print(\"already processed this country held out\")\n",
    "        df_images = pd.read_csv(savepath)\n",
    "        return df_images\n",
    "    df_images = DF_DOWNLOAD.copy()\n",
    "    df_images['is_train'] = True\n",
    "    # these belong to the country held out\n",
    "    df_images['is_train'].loc[df_images['country'] == COUNTRY] = False\n",
    "    create_bin(df_images, METRIC)\n",
    "    \n",
    "    os.makedirs(os.path.join(CNN_TRAIN_IMAGE_DIR, 'train'), exist_ok=False)\n",
    "    os.makedirs(os.path.join(CNN_TRAIN_IMAGE_DIR, 'valid'), exist_ok=False)\n",
    "    \n",
    "    symlink_images(df_images)\n",
    "    \n",
    "    # save to disk\n",
    "    df_images.to_csv(savepath, index=False)\n",
    "    return df_images\n",
    "    \n",
    "def symlink_images(df_images):\n",
    "    '''\n",
    "    df_images: dataframe with 'image_name', 'country', 'is_train' columns\n",
    "    \n",
    "    This function will symlink (a type of link that takes very little space and points to another link)\n",
    "    the images into \"train\" and \"valid\" folders in CNN_TRAIN_IMAGE_DIR\n",
    "    Symlinking prevents us from having to copy the images, which saves disk space and time. From a user's\n",
    "    perspective, opening the symlinked file opens the actual hard link file elsewhere. This means\n",
    "    our CNN training can operate on a directory of symlinked images without any problem/knowledge of\n",
    "    symlinks because this function is supported natively by the filesystem. \n",
    "    In this case, the original hard link is in the original download directory at COUNTRIES_DIR/<country>/cnn_images.\n",
    "    THAT DIRECTORY CANNOT BE MOVED OR MODIFIED OR SCRIPTS WILL BREAK\n",
    "    '''\n",
    "    train = df_images[df_images['is_train']]\n",
    "    valid = df_images[~df_images['is_train']]\n",
    "    \n",
    "    # uses symlinking to save disk space\n",
    "    print('symlinking train images')\n",
    "    for im_name, country in tqdm(zip(train['image_name'], train['country']), total=len(train)):\n",
    "        src = os.path.abspath(os.path.join(COUNTRIES_DIR, country, 'cnn_images', im_name))\n",
    "        dest = os.path.join(CNN_TRAIN_IMAGE_DIR, 'train', im_name)\n",
    "        if os.system(f\"ln -s {src} {dest}\") != 0:\n",
    "            print(\"error creating symlink\")\n",
    "            raise ValueError()\n",
    "\n",
    "    print('symlinking valid images')\n",
    "    for im_name, country in tqdm(zip(valid['image_name'], valid['country']), total=len(valid)):\n",
    "        src = os.path.abspath(os.path.join(COUNTRIES_DIR, country, 'cnn_images', im_name))\n",
    "        dest = os.path.join(CNN_TRAIN_IMAGE_DIR, 'valid', im_name)\n",
    "        if os.system(f\"ln -s {src} {dest}\") != 0:\n",
    "            print(\"error creating symlink\")\n",
    "            raise ValueError()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already processed this country\n"
     ]
    }
   ],
   "source": [
    "df_images = None\n",
    "if TYPE == 'single_country':\n",
    "    df_images = preprocess_single_country(frac=0.7)\n",
    "else:\n",
    "    df_images = preprocess_country_held_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_lat</th>\n",
       "      <th>image_lon</th>\n",
       "      <th>cluster_lat</th>\n",
       "      <th>cluster_lon</th>\n",
       "      <th>house_has_cellphone</th>\n",
       "      <th>est_monthly_phone_cost_pc</th>\n",
       "      <th>country</th>\n",
       "      <th>is_train</th>\n",
       "      <th>bin</th>\n",
       "      <th>near_lower</th>\n",
       "      <th>near_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-17.140065764205975_35.17229723579403_-17.0951...</td>\n",
       "      <td>-17.140066</td>\n",
       "      <td>35.172297</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.819316</td>\n",
       "      <td>malawi_2016</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-17.11012192140199_35.17229723579403_-17.09515...</td>\n",
       "      <td>-17.110122</td>\n",
       "      <td>35.172297</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.819316</td>\n",
       "      <td>malawi_2016</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-17.08017807859801_35.17229723579403_-17.09515...</td>\n",
       "      <td>-17.080178</td>\n",
       "      <td>35.172297</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.819316</td>\n",
       "      <td>malawi_2016</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-17.050234235794026_35.17229723579403_-17.0951...</td>\n",
       "      <td>-17.050234</td>\n",
       "      <td>35.172297</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.819316</td>\n",
       "      <td>malawi_2016</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-17.140065764205975_35.20224107859801_-17.0951...</td>\n",
       "      <td>-17.140066</td>\n",
       "      <td>35.202241</td>\n",
       "      <td>-17.09515</td>\n",
       "      <td>35.217213</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.819316</td>\n",
       "      <td>malawi_2016</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_name  image_lat  image_lon  \\\n",
       "0  -17.140065764205975_35.17229723579403_-17.0951... -17.140066  35.172297   \n",
       "1  -17.11012192140199_35.17229723579403_-17.09515... -17.110122  35.172297   \n",
       "2  -17.08017807859801_35.17229723579403_-17.09515... -17.080178  35.172297   \n",
       "3  -17.050234235794026_35.17229723579403_-17.0951... -17.050234  35.172297   \n",
       "4  -17.140065764205975_35.20224107859801_-17.0951... -17.140066  35.202241   \n",
       "\n",
       "   cluster_lat  cluster_lon  house_has_cellphone  est_monthly_phone_cost_pc  \\\n",
       "0    -17.09515    35.217213                  0.5                   0.819316   \n",
       "1    -17.09515    35.217213                  0.5                   0.819316   \n",
       "2    -17.09515    35.217213                  0.5                   0.819316   \n",
       "3    -17.09515    35.217213                  0.5                   0.819316   \n",
       "4    -17.09515    35.217213                  0.5                   0.819316   \n",
       "\n",
       "       country  is_train  bin  near_lower  near_upper  \n",
       "0  malawi_2016      True    2       False       False  \n",
       "1  malawi_2016      True    2       False       False  \n",
       "2  malawi_2016      True    2       False       False  \n",
       "3  malawi_2016      True    2       False       False  \n",
       "4  malawi_2016      True    2       False       False  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_images.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using existing model at epoch 29\n"
     ]
    }
   ],
   "source": [
    "def initialize_model():\n",
    "    global CURRENT_EPOCH\n",
    "    model = None\n",
    "    input_size = 224 # hardcoded for VGG, our network\n",
    "    existing = os.listdir(CNN_SAVE_DIR)\n",
    "    found = False\n",
    "    if len(existing) != 0:\n",
    "        largest_epoch = 0\n",
    "        prefix = f'trained_model_{METRIC}_epoch_'\n",
    "        for f in existing:\n",
    "            if f[:len(prefix)] != prefix:\n",
    "                continue\n",
    "            found = True\n",
    "            string = f.split('.')[0] # remove extension\n",
    "            epoch = int(string[len(prefix):]) # parse out the epoch\n",
    "            if epoch > largest_epoch:\n",
    "                largest_epoch = epoch\n",
    "        if found:\n",
    "            CURRENT_EPOCH = largest_epoch + 1\n",
    "            path = os.path.join(CNN_SAVE_DIR, prefix + str(largest_epoch) + '.pt')\n",
    "            model = torch.load(path, map_location=DEVICE)\n",
    "            print(f'using existing model at epoch {largest_epoch}')\n",
    "    if not found:\n",
    "        torch.manual_seed(RANDOM_SEED)\n",
    "        model = models.vgg11_bn(pretrained=True)\n",
    "        # turn off training for all existing paramaters (for now)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        num_ftrs = model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Linear(num_ftrs, NUMBER_OF_BINS)\n",
    "        model = model.to(DEVICE)\n",
    "    return model, input_size\n",
    "\n",
    "model, input_size = initialize_model()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will query this to figure out the correct label\n",
    "DF_LOOKUP = df_images.set_index('image_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "class ForwardPassDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, transformer):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_list = os.listdir(self.image_dir)\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.image_list[index]\n",
    "\n",
    "        # Load image\n",
    "        X = self.filename_to_im_tensor(self.image_dir + '/' + image_name)\n",
    "        y = DF_LOOKUP.loc[image_name]['bin']\n",
    "        \n",
    "        return X, y, image_name\n",
    "    \n",
    "    def filename_to_im_tensor(self, file):\n",
    "        im = (plt.imread(file)[:,:,:3] * 256).astype(np.uint8)\n",
    "        im = Image.fromarray(im)\n",
    "        im = self.transformer(im)\n",
    "        return im\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: ForwardPassDataset(os.path.join(CNN_TRAIN_IMAGE_DIR, x), \n",
    "                                          data_transforms[x]) for x in ['train', 'valid']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                                                   batch_size=BATCH_SIZE, \n",
    "                                                   shuffle=True, \n",
    "                                                   num_workers=4) for x in ['train', 'valid']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCriterion:\n",
    "    '''\n",
    "    This custom criterion will allow images that are near the border of two bins\n",
    "    to calculate their loss partially based on the bin they are close to\n",
    "    '''\n",
    "    def __init__(self, alpha=0.75):\n",
    "        # alpha describes what percent should go to the correct class\n",
    "        # if the image is near_lower or near_upper\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def __call__(self, outputs, labels, image_names):\n",
    "        ret = None\n",
    "        for i in range(len(image_names)):\n",
    "            lookup = DF_LOOKUP.loc[image_names[i]]\n",
    "            output = outputs[i].reshape(1, -1)\n",
    "            label = labels[i].reshape(1)\n",
    "           \n",
    "            if lookup['near_upper']:\n",
    "                # the +1 on the second line shifts the criteria to the upper bin\n",
    "                iret = self.alpha * self.criterion(output, label) + \\\n",
    "                        (1 - self.alpha) * self.criterion(output, label + 1)\n",
    "            elif lookup['near_lower']:\n",
    "                # the -1 on the second line shifts the criteria to the lower bin\n",
    "                iret = self.alpha * self.criterion(output, label) + \\\n",
    "                        (1 - self.alpha) * self.criterion(output, label - 1)\n",
    "            else:\n",
    "                iret = self.criterion(output, label) # regular cross entropy\n",
    "            if ret is None:\n",
    "                ret = iret\n",
    "            else:\n",
    "                ret += iret\n",
    "        return ret / len(image_names) # averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs):\n",
    "    global CURRENT_EPOCH, DEVICE\n",
    "    since = time.time()\n",
    "    val_acc_history = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(CURRENT_EPOCH, num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        if epoch == 5:\n",
    "            # fine tune whole model now\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels, image_names in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # track gradients in train phase only\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels, image_names)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            # deep copy the model if it is better\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'valid':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        if epoch % 5 == 4:\n",
    "            # save intermediate results in case script breaks\n",
    "            savepath = os.path.join(CNN_SAVE_DIR, f'trained_model_{METRIC}_epoch_{epoch}.pt')\n",
    "            torch.save(model, savepath)\n",
    "        \n",
    "        # end one epoch\n",
    "        CURRENT_EPOCH += 1\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    # load best model\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/29\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77de57fb7b5b4de88a82c815d797898c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.1240 Acc: 0.5077\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959b67ee2bef470ca87ed64098532872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.4982 Acc: 0.2943\n",
      "\n",
      "Epoch 16/29\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d80de28d58423785e54e689074c798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.1199 Acc: 0.5102\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd157f98967466ba8acf03fca36a7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.5119 Acc: 0.3032\n",
      "\n",
      "Epoch 17/29\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3aaa87ee284892b407492b6e6c2781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.1049 Acc: 0.5162\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5bc0c5bb555435584021dd4db03ff91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.4924 Acc: 0.2993\n",
      "\n",
      "Epoch 18/29\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "950ae51abf854c19a4995d33c7aff3ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0992 Acc: 0.5218\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbc86cf402d4126a2496560a7551fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.5238 Acc: 0.2761\n",
      "\n",
      "Epoch 19/29\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200967b7759d43e6801ab4ac097817eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0942 Acc: 0.5220\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d37de396f44cfdb70c9889f12a76d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.5111 Acc: 0.2995\n",
      "\n",
      "Epoch 20/29\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907243a8c4d442e19ae06e6af752918b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0873 Acc: 0.5232\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d4988c751d4354948980b8460a1764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.4998 Acc: 0.3035\n",
      "\n",
      "Epoch 21/29\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddada08470ba446bbb5dd594419aa148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0794 Acc: 0.5321\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1342e4cb7f244e3c96967808d109e932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.5211 Acc: 0.2977\n",
      "\n",
      "Epoch 22/29\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f6cb35bce341eeb91a25a0d987440a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0663 Acc: 0.5375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22d6ee4eecd490ca86547b9772a21c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.5518 Acc: 0.2761\n",
      "\n",
      "Epoch 23/29\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7130beebc4443dd980e5008cb6c9c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0659 Acc: 0.5421\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ca1086b8cb46ef8f1a92615a9307e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.5374 Acc: 0.2761\n",
      "\n",
      "Epoch 24/29\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cece5a6c2b9b414aa2eda446bc68ab93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0642 Acc: 0.5419\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8e919da9844fda915e0c6c65a0a89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.5573 Acc: 0.2870\n",
      "\n",
      "Epoch 25/29\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8dd49be0fb94ae38600604632ac5ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0523 Acc: 0.5511\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def8cad8c5324f588b614b16c078eef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.5596 Acc: 0.2748\n",
      "\n",
      "Epoch 26/29\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58205de41c1a4d5899b72d910870efeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0412 Acc: 0.5564\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22472036c464cb7ae94d045ec047378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.5584 Acc: 0.2733\n",
      "\n",
      "Epoch 27/29\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895be25fb5524195952b3f829e077b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0376 Acc: 0.5575\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1fb609aa5bd40d2b2ab098edc7a5939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.5918 Acc: 0.2698\n",
      "\n",
      "Epoch 28/29\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78abb100f76347f9a51b369e683aa1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0326 Acc: 0.5560\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d724782a6bf54c9180fcb2c54639a41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.5707 Acc: 0.2843\n",
      "\n",
      "Epoch 29/29\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1fcb2edf26443bb28d53c40beb4fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/330 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0252 Acc: 0.5627\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7815de1f14d046e0ab456ea6879a70f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 1.6286 Acc: 0.2611\n",
      "\n",
      "Training complete in 76m 29s\n",
      "Best val Acc: 0.303456\n"
     ]
    }
   ],
   "source": [
    "criterion = CustomCriterion()\n",
    "model, hist = train_model(model, dataloaders_dict, criterion, optimizer, TOTAL_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ../models/country_held_out/malawi_2016/house_has_cellphone/trained_model_house_has_cellphone.pt\n"
     ]
    }
   ],
   "source": [
    "savepath = os.path.join(CNN_SAVE_DIR, f'trained_model_{METRIC}.pt')\n",
    "if os.path.isfile(savepath):\n",
    "    print('A model is already saved at this location')\n",
    "else:\n",
    "    print(f'Saving model to {savepath}')\n",
    "    torch.save(model, savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(os.path.join(CNN_SAVE_DIR, f'trained_model_{METRIC}.pt'), map_location=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3088607fab24b9c9ed5ea064b82fb9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.4305\n"
     ]
    }
   ],
   "source": [
    "# use this to see the validation accuracy\n",
    "model.eval()\n",
    "running_corrects = 0\n",
    "for inputs, labels, _ in tqdm(dataloaders_dict['valid']):\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    # statistics\n",
    "    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "epoch_acc = running_corrects.double() / len(dataloaders_dict['valid'].dataset)\n",
    "\n",
    "print('Acc: {:.4f}'.format(epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "taddle",
   "language": "python",
   "name": "taddle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
